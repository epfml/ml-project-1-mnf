{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from helpers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='./dataset_to_release/'\n",
    "train_data_path=\"./dataset_to_release/x_train.csv\"\n",
    "test_data_path=\"./dataset_to_release/x_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids=load_csv_data_all(data_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_path=\"./dataset/x_train.csv\"\n",
    "# train_label_path=\"./dataset/y_train.csv\"\n",
    "# train_data=pd.read_csv(train_data_path)\n",
    "# train_label=pd.read_csv(train_label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data points\n",
    "# xb=load_csv_data(train_data_path,sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the labels \n",
    "# yb=load_csv_data_labels(train_label_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# y = np.expand_dims(yb, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the column names from the x_train.csv and convert it into a list\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handeling the missing values\n",
    "\n",
    "def replace_nan_by_mean(data):\n",
    "    ''' function that handels the missing values by replacing them with the column means'''\n",
    "    nan_indices = np.isnan(data)\n",
    "    column_means = np.nanmean(data, axis=0)\n",
    "    data[nan_indices] = np.take(column_means, np.where(nan_indices)[1])\n",
    "    return data\n",
    "\n",
    "\n",
    "data_train = replace_nan_by_mean(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verifying that the nan values have been successfully removed\n",
    "# matrix=np.isnan(data_train)\n",
    "# num_true=np.count_nonzero(matrix)\n",
    "# num_false = matrix.size - num_true\n",
    "# print(num_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135, 321)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135, 321)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data filtering: we only keep relevent features\n",
    "\n",
    "def filtering(data, data_path):\n",
    "    columns = extract_first_line(data_path).split(',')\n",
    "    columns.pop(0)\n",
    "    columns_to_keep = []\n",
    "    for c in columns:\n",
    "        if c.startswith('_'):\n",
    "            columns_to_keep.append(c)\n",
    "    indices_to_keep = [columns.index(c) for c in columns_to_keep]\n",
    "    data_f = data[:, indices_to_keep]\n",
    "    return(data_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = [\"_AGE80\", \"_AGE65YR\", \"_AGEG5YR\", \"_AGE_G\", \"_AIDTST3\", \"_ASTHMS1\", \"_BMI5\", \"_BMI5CAT\", \"_CASTHM1\", \"_CHLDCNT\", \"_CHOLCHK\", \"_DRDXAR1\", \"_DRNKWEK\", \"_DUALCOR\", \"_DUALUSE\", \"_EDUCAG\", \"_FLSHOT6\", \"_FRT16\", \"_FRTLT1\", \"_FRTRESP\", \"_FRUITEX\", \"_FRUTSUM\", \"_HCVU651\", \"_HISPANC\", \"_INCOMG\", \"_LLCPWT\", \"_LMTACT1\", \"_LMTSCL1\", \"_LMTWRK1\", \"_LTASTH1\", \"_MICHD\", \"_MINAC11\", \"_MINAC21\", \"_MISFRTN\", \"_MISVEGN\", \"_MRACE1\", \"_PA30021\", \"_PA150R2\", \"_PA300R2\", \"_PACAT1\", \"_PAINDX1\", \"_PAREC1\", \"_PASTAE1\", \"_PASTRNG\", \"_PNEUMO2\", \"_PRACE1\", \"_RACE\", \"_RACEG21\", \"_RACEGR3\", \"_RACE_G1\", \"_RFBING5\", \"_RFBMI5\", \"_RFCHOL\", \"_RFDRHV5\", \"_RFHLTH\", \"_RFHYPE5\", \"_RFSEAT2\", \"_RFSEAT3\", \"_RFSMOK3\", \"_SMOKER3\", \"_TOTINDA\", \"_VEG23\", \"_VEGESUM\", \"_VEGETEX\", \"_VEGLT1\", \"_VEGRESP\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secode version of data filtering, remove 9 more columns\n",
    "def filtering_2(data,data_path):\n",
    "    columns = extract_first_line(data_path).split(',')\n",
    "    columns.pop(0)\n",
    "    filtered_columns = [col for col in columns if col in features_to_keep]\n",
    "    indices_to_keep = [columns.index(c) for c in filtered_columns]\n",
    "    print(len(indices_to_keep))\n",
    "\n",
    "    data_f = data[:, indices_to_keep]\n",
    "    return(data_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "data_train_filtered_2=filtering_2(data_train, train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135, 65)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_filtered_2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization of the data\n",
    "def standardize(data):\n",
    "    small_value=1*10**(-9)\n",
    "    mean=np.mean(data, axis=0)\n",
    "    std=np.std(data, axis=0)+small_value\n",
    "    return((data - mean) / (std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train_standard=standardize(data_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature augmentation\n",
    "def feature_expansion(data, degree):\n",
    "    augmented_features=[]\n",
    "    for i in range(data.shape[1]):\n",
    "        feature=data[:,i]\n",
    "        augmented_feature=build_poly(feature, degree)\n",
    "        augmented_features.append(augmented_feature)\n",
    "\n",
    "    # Stack the augmented features horizontally\n",
    "    augmented_data = np.hstack(augmented_features)\n",
    "    return(augmented_data)\n",
    "\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Computes the F1 score for a classification model using NumPy.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels (numpy.ndarray): True labels for the data.\n",
    "    predicted_labels (numpy.ndarray): Predicted labels from the model.\n",
    "\n",
    "    Returns:\n",
    "    f1 (float): The F1 score.\n",
    "    \"\"\"\n",
    "    true_positive = np.sum(np.logical_and(true_labels == 1, predicted_labels == 1))\n",
    "    false_positive = np.sum(np.logical_and(true_labels == 0, predicted_labels == 1))\n",
    "    false_negative = np.sum(np.logical_and(true_labels == 1, predicted_labels == 0))\n",
    "    \n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "\n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)]\n",
    "    \n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(test, model):\n",
    "    pred=(sigmoid(test.dot(model))>=0.2).astype(int)\n",
    "    return(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135, 75)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\n",
    "\n",
    "    Args:\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        degree:     scalar, cf. build_poly()\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # get k'th subgroup in test, others in train:\n",
    "    train_idx=np.reshape(k_indices[[i for i in range(len(k_indices)) if i!=k]], -1)\n",
    "    test_idx=k_indices[k]\n",
    "\n",
    "    x_train=x[train_idx,:]\n",
    "    print(x_train.shape)\n",
    "    y_train=y[train_idx]\n",
    "    x_test=x[test_idx,:]\n",
    "    y_test=y[test_idx]\n",
    "    \n",
    "    y_tr=np.expand_dims(y_train, 1)\n",
    "    y_te=np.expand_dims(y_test, 1)\n",
    "\n",
    "    y_tr=np.where(y_tr == -1, 0, y_tr)\n",
    "    print(y_tr, y_tr.shape)\n",
    "    y_te=np.where(y_te == -1, 0, y_te)\n",
    "\n",
    "    max_iters = 1000\n",
    "    gamma=0.5\n",
    "\n",
    "    # ***************************************************\n",
    "    # form data with polynomial degree: \n",
    "    print('on va auggmenter le data')\n",
    "    train_data=feature_expansion(x_train, degree)\n",
    "    test_data=feature_expansion(x_test, degree)\n",
    "    train_data=standardize(train_data)\n",
    "    test_data=standardize(test_data)\n",
    "    # ***************************************************\n",
    "     # build tx\n",
    "    tx_tr = np.c_[np.ones((y_train.shape[0], 1)), train_data]\n",
    "    tx_te = np.c_[np.ones((test_data.shape[0], 1)), test_data]\n",
    "    print(tx_tr.shape)\n",
    "    print(tx_te.shape)\n",
    "    initial_w=np.zeros((tx_tr.shape[1], 1))\n",
    "\n",
    "    # reg logistic regression: \n",
    "    w=reg_logistic_regression(y_tr,tx_tr,lambda_,initial_w, max_iters, gamma)[0]\n",
    "    print(w.shape)\n",
    "    print(tx_te.shape)\n",
    "    y_pred=apply_model(tx_te, w)\n",
    "    # calculate f1 score on test:\n",
    "    f1_te=compute_f1_score(y_te, y_pred)\n",
    "  \n",
    "    return f1_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_demo(degree, k_fold, lambda_):\n",
    "    \"\"\"cross validation over regularisation parameter lambda.\n",
    "\n",
    "    Args:\n",
    "        degree: integer, degree of the polynomial expansion\n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : scalar, the associated root mean squared error for the best lambda\n",
    "    \"\"\"\n",
    "\n",
    "    seed = 12\n",
    "    #degree = degree\n",
    "    k_fold = k_fold\n",
    "    lambda_ = lambda_\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    f1_score=[]\n",
    "    # cross validation over lambdas:\n",
    "    for d in degree:\n",
    "        cross_val=[cross_validation(y_train, data_train_filtered, k_indices, k, lambda_, d) for k in range(k_fold)]\n",
    "        f1=np.mean(cross_val)\n",
    "        f1_score.append(f1)\n",
    "    print('on y est presque')\n",
    "    best_degree=degree[np.argmax(f1_score)]\n",
    "    best_f1=np.min(f1_score)\n",
    "    # print(\n",
    "    #     \"For polynomial expansion up to degree %.f, the choice of lambda which leads to the best test rmse is %.5f with a test rmse of %.3f\"\n",
    "    #     % (degree, best_lambda, f1_score)\n",
    "    # )\n",
    "    return best_degree, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164067, 75)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] (164067, 1)\n",
      "on va auggmenter le data\n",
      "(164067, 151)\n",
      "(164067, 151)\n",
      "Current iteration=0, loss=0.602928317667096\n",
      "Current iteration=100, loss=0.24995198845399821\n",
      "Current iteration=200, loss=0.24839048464870833\n",
      "Current iteration=300, loss=0.2481155255834338\n",
      "Current iteration=400, loss=0.24803562715193608\n",
      "Current iteration=500, loss=0.24800115758122054\n",
      "Current iteration=600, loss=0.24797971970665422\n",
      "Current iteration=700, loss=0.24796310425383883\n",
      "Current iteration=800, loss=0.24794895128918823\n",
      "Current iteration=900, loss=0.24793644759737749\n",
      "loss=0.24792533450197074\n",
      "(151, 1)\n",
      "(164067, 151)\n",
      "(164067, 75)\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]] (164067, 1)\n",
      "on va auggmenter le data\n",
      "(164067, 151)\n",
      "(164067, 151)\n",
      "Current iteration=0, loss=0.6031193721524989\n",
      "Current iteration=100, loss=0.25165567787123\n",
      "Current iteration=200, loss=0.25006465583741655\n",
      "Current iteration=300, loss=0.24978007723228135\n",
      "Current iteration=400, loss=0.24969841484790592\n",
      "Current iteration=500, loss=0.2496643769019261\n",
      "Current iteration=600, loss=0.24964390564696404\n",
      "Current iteration=700, loss=0.24962831236684208\n",
      "Current iteration=800, loss=0.24961509605136098\n",
      "Current iteration=900, loss=0.2496034049856027\n",
      "loss=0.24959296880847018\n",
      "(151, 1)\n",
      "(164067, 151)\n",
      "(164067, 75)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] (164067, 1)\n",
      "on va auggmenter le data\n",
      "(164067, 226)\n",
      "(164067, 226)\n",
      "Current iteration=0, loss=0.6023164319515367\n",
      "Current iteration=100, loss=0.24287481918325438\n",
      "Current iteration=200, loss=0.23889591298332988\n",
      "Current iteration=300, loss=0.23774613328758926\n",
      "Current iteration=400, loss=0.23728074355817394\n",
      "Current iteration=500, loss=0.2370423024545407\n",
      "Current iteration=600, loss=0.23689305146656311\n",
      "Current iteration=700, loss=0.23678457677225198\n",
      "Current iteration=800, loss=0.23669780782117786\n",
      "Current iteration=900, loss=0.23662436529023026\n",
      "loss=0.23656073258439458\n",
      "(226, 1)\n",
      "(164067, 226)\n",
      "(164067, 75)\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]] (164067, 1)\n",
      "on va auggmenter le data\n",
      "(164067, 226)\n",
      "(164067, 226)\n",
      "Current iteration=0, loss=0.6024373401743929\n",
      "Current iteration=100, loss=0.24441854364256466\n",
      "Current iteration=200, loss=0.24043919886536447\n",
      "Current iteration=300, loss=0.23928997943961158\n",
      "Current iteration=400, loss=0.23881559768461486\n",
      "Current iteration=500, loss=0.23856518484320494\n",
      "Current iteration=600, loss=0.23840468177621346\n",
      "Current iteration=700, loss=0.23828683789600513\n",
      "Current iteration=800, loss=0.2381926218807655\n",
      "Current iteration=900, loss=0.23811333255659764\n",
      "loss=0.23804512471347872\n",
      "(226, 1)\n",
      "(164067, 226)\n",
      "(164067, 75)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] (164067, 1)\n",
      "on va auggmenter le data\n",
      "(164067, 301)\n",
      "(164067, 301)\n",
      "Current iteration=0, loss=0.6079614103142099\n",
      "Current iteration=100, loss=0.2411165717433561\n",
      "Current iteration=200, loss=0.23774427260347714\n",
      "Current iteration=300, loss=0.2369718228128543\n",
      "Current iteration=400, loss=0.23672972868163286\n",
      "Current iteration=500, loss=0.23658566762695799\n",
      "Current iteration=600, loss=0.23647579651363937\n",
      "Current iteration=700, loss=0.23638543507700518\n",
      "Current iteration=800, loss=0.23630849637263202\n",
      "Current iteration=900, loss=0.2362417177038386\n",
      "loss=0.23618357014304955\n",
      "(301, 1)\n",
      "(164067, 301)\n",
      "(164067, 75)\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]] (164067, 1)\n",
      "on va auggmenter le data\n",
      "(164067, 301)\n",
      "(164067, 301)\n",
      "Current iteration=0, loss=0.6080628719254632\n",
      "Current iteration=100, loss=0.2430478579996611\n",
      "Current iteration=200, loss=0.2391239208682942\n",
      "Current iteration=300, loss=0.2384661606324876\n",
      "Current iteration=400, loss=0.23821867637506766\n",
      "Current iteration=500, loss=0.2380618479434423\n",
      "Current iteration=600, loss=0.23794441370446837\n",
      "Current iteration=700, loss=0.23784951950061747\n",
      "Current iteration=800, loss=0.23776981460622798\n",
      "Current iteration=900, loss=0.23770133899924673\n",
      "loss=0.23764217767576054\n",
      "(301, 1)\n",
      "(164067, 301)\n",
      "(164067, 75)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] (164067, 1)\n",
      "on va auggmenter le data\n",
      "(164067, 376)\n",
      "(164067, 376)\n",
      "Current iteration=0, loss=0.6185388677725671\n",
      "Current iteration=100, loss=0.24302497288468192\n",
      "Current iteration=200, loss=0.24076243008871934\n",
      "Current iteration=300, loss=0.2402617771027523\n",
      "Current iteration=400, loss=0.24003035274359807\n",
      "Current iteration=500, loss=0.23987940235750263\n",
      "Current iteration=600, loss=0.23976508100801902\n",
      "Current iteration=700, loss=0.23967251996573233\n",
      "Current iteration=800, loss=0.2395949406931565\n",
      "Current iteration=900, loss=0.23952851861896093\n",
      "loss=0.24473285902578248\n",
      "(376, 1)\n",
      "(164067, 376)\n",
      "(164067, 75)\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]] (164067, 1)\n",
      "on va auggmenter le data\n",
      "(164067, 376)\n",
      "(164067, 376)\n",
      "Current iteration=0, loss=0.6186228467753195\n",
      "Current iteration=100, loss=0.2521880054718214\n",
      "Current iteration=200, loss=0.24792373125165384\n",
      "Current iteration=300, loss=0.24684494786716651\n",
      "Current iteration=400, loss=0.24639817795046404\n",
      "Current iteration=500, loss=0.24615634739902847\n",
      "Current iteration=600, loss=0.24600000282112489\n",
      "Current iteration=700, loss=0.24588652958740015\n",
      "Current iteration=800, loss=0.24579794699537502\n",
      "Current iteration=900, loss=0.24572553871723654\n",
      "loss=0.24081787941581062\n",
      "(376, 1)\n",
      "(164067, 376)\n",
      "(164067, 75)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] (164067, 1)\n",
      "on va auggmenter le data\n",
      "(164067, 451)\n",
      "(164067, 451)\n",
      "Current iteration=0, loss=0.6329123947399634\n",
      "Current iteration=100, loss=0.282076479178285\n",
      "Current iteration=200, loss=0.2730360888730939\n",
      "Current iteration=300, loss=0.2680110738715318\n",
      "Current iteration=400, loss=0.2686138324279587\n",
      "Current iteration=500, loss=0.2678731483357913\n",
      "Current iteration=600, loss=0.26768585065221534\n",
      "Current iteration=700, loss=0.26753106749137107\n",
      "Current iteration=800, loss=0.2674213732131266\n",
      "Current iteration=900, loss=0.26733472508553446\n",
      "loss=0.24440949112625218\n",
      "(451, 1)\n",
      "(164067, 451)\n",
      "(164067, 75)\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]] (164067, 1)\n",
      "on va auggmenter le data\n",
      "(164067, 451)\n",
      "(164067, 451)\n",
      "Current iteration=0, loss=0.6329709716836909\n",
      "Current iteration=100, loss=0.24984633381684596\n",
      "Current iteration=200, loss=0.24708263948793308\n",
      "Current iteration=300, loss=0.2464908725383324\n",
      "Current iteration=400, loss=0.24620076642324645\n",
      "Current iteration=500, loss=0.2460227922978968\n",
      "Current iteration=600, loss=0.24589679874574608\n",
      "Current iteration=700, loss=0.2457999584900667\n",
      "Current iteration=800, loss=0.2457217486728014\n",
      "Current iteration=900, loss=0.24565654898776412\n",
      "loss=0.24487418824529048\n",
      "(451, 1)\n",
      "(164067, 451)\n",
      "on y est presque\n"
     ]
    }
   ],
   "source": [
    "best_lambda, best_f1 = cross_validation_demo(np.array([1,2,3,4,5]).astype(int), 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03153655475434211"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lambda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data=feature_expansion(data_train_filtered_2, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization of the data\n",
    "def standardize(data):\n",
    "    small_value=1*10**(-9)\n",
    "    mean=np.mean(data, axis=0)\n",
    "    std=np.std(data, axis=0)+small_value\n",
    "    return((data - mean) / (std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.30000000e+01, 2.01501563e+09, 5.32049000e+05, ...,\n",
       "        2.28990981e+00, 2.40679360e+00, 2.00000000e+00],\n",
       "       [3.30000000e+01, 2.01500439e+09, 3.31071000e+05, ...,\n",
       "        2.28990981e+00, 2.40679360e+00, 1.96667511e+00],\n",
       "       [2.00000000e+01, 2.01500564e+09, 2.01091000e+05, ...,\n",
       "        1.00000000e+00, 2.00000000e+00, 2.00000000e+00],\n",
       "       ...,\n",
       "       [3.90000000e+01, 2.01500490e+09, 3.91012000e+05, ...,\n",
       "        2.00000000e+00, 2.00000000e+00, 2.00000000e+00],\n",
       "       [3.30000000e+01, 2.01500445e+09, 3.31072000e+05, ...,\n",
       "        2.28990981e+00, 2.40679360e+00, 2.00000000e+00],\n",
       "       [3.20000000e+01, 2.01500118e+09, 3.21011000e+05, ...,\n",
       "        2.28990981e+00, 2.40679360e+00, 2.00000000e+00]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standardized = standardize(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135, 276)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standardized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the test set in two\n",
    "\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8\n",
    "    you will have 80% of your data set dedicated to training\n",
    "    and the rest dedicated to testing. If ratio times the number of samples is not round\n",
    "    you can use np.floor. Also check the documentation for np.random.permutation,\n",
    "    it could be useful.\n",
    "\n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        y: numpy array of shape (N,).\n",
    "        ratio: scalar in [0,1]\n",
    "        seed: integer.\n",
    "\n",
    "    Returns:\n",
    "        x_tr: numpy array containing the train data.\n",
    "        x_te: numpy array containing the test data.\n",
    "        y_tr: numpy array containing the train labels.\n",
    "        y_te: numpy array containing the test labels.\n",
    "    \"\"\"\n",
    "    N=int(ratio*len(x))\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # split the data based on the given ratio: \n",
    "    shuffled_data=np.random.permutation(x)\n",
    "    #print(shuffled_data)\n",
    "    np.random.seed(seed)\n",
    "    shuffled_labels=np.random.permutation(y)\n",
    "    #print(shuffled_labels)\n",
    "    x_tr=shuffled_data[:N] #train data\n",
    "    x_te=shuffled_data[N:] #test data\n",
    "    y_tr=shuffled_labels[:N]#train labels\n",
    "    y_te=shuffled_labels[N:]# test labels\n",
    "\n",
    "    return(x_tr,x_te, y_tr, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr,x_te, y_tr, y_te=split_data(data_standardized, y_train, ratio=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262508, 1) (262508, 276)\n"
     ]
    }
   ],
   "source": [
    "print(y_tr.shape, x_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train=np.expand_dims(y_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135, 1, 1)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification using logistic regression\n",
    "\n",
    "max_iters = 10000\n",
    "gamma = 0.5\n",
    "\n",
    " # build tx\n",
    "tx_tr = np.c_[np.ones((y_tr.shape[0], 1)), x_tr]\n",
    "initial_w=np.zeros((tx_tr.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train == -1, 0, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad=calculate_log_likelihood_gradient(y_train,tx_tr,initial_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w=initial_w-gamma*grad\n",
    "# print(np.min(w), np.max(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss=calculate_log_likelihood_loss(y_train,tx_tr,initial_w)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6046881574088626\n",
      "Current iteration=100, loss=0.2523966461566142\n",
      "Current iteration=200, loss=0.24975666071294256\n",
      "Current iteration=300, loss=0.24882370999293232\n",
      "Current iteration=400, loss=0.24856549904230596\n",
      "Current iteration=500, loss=0.24843893361640268\n",
      "Current iteration=600, loss=0.24833554114888434\n",
      "Current iteration=700, loss=0.24824505245815223\n",
      "Current iteration=800, loss=0.2481647170626667\n",
      "Current iteration=900, loss=0.248092918219218\n",
      "Current iteration=1000, loss=0.2480284636159432\n",
      "Current iteration=1100, loss=0.24797039679946647\n",
      "Current iteration=1200, loss=0.24791792257985393\n",
      "Current iteration=1300, loss=0.24787036835776782\n",
      "Current iteration=1400, loss=0.24782715960604917\n",
      "Current iteration=1500, loss=0.24778780230292832\n",
      "Current iteration=1600, loss=0.24775186947921538\n",
      "Current iteration=1700, loss=0.24771899053553295\n",
      "Current iteration=1800, loss=0.24768884257904317\n",
      "Current iteration=1900, loss=0.24766114330416086\n",
      "Current iteration=2000, loss=0.24763564508856234\n",
      "Current iteration=2100, loss=0.247612130063812\n",
      "Current iteration=2200, loss=0.24759040597757362\n",
      "Current iteration=2300, loss=0.2475703027046371\n",
      "Current iteration=2400, loss=0.2475516692934324\n",
      "Current iteration=2500, loss=0.24753437145694632\n",
      "Current iteration=2600, loss=0.24751828943414814\n",
      "Current iteration=2700, loss=0.24750331616156163\n",
      "Current iteration=2800, loss=0.24748935570536593\n",
      "Current iteration=2900, loss=0.24747632191307475\n",
      "Current iteration=3000, loss=0.2474641372508341\n",
      "Current iteration=3100, loss=0.247452731798084\n",
      "Current iteration=3200, loss=0.24744204237599893\n",
      "Current iteration=3300, loss=0.24743201178996072\n",
      "Current iteration=3400, loss=0.2474225881694754\n",
      "Current iteration=3500, loss=0.2474137243915764\n",
      "Current iteration=3600, loss=0.24740537757592343\n",
      "Current iteration=3700, loss=0.2473975086416297\n",
      "Current iteration=3800, loss=0.24739008191735073\n",
      "Current iteration=3900, loss=0.24738306479744662\n",
      "Current iteration=4000, loss=0.24737642743808685\n",
      "Current iteration=4100, loss=0.2473701424880617\n",
      "Current iteration=4200, loss=0.24736418484982087\n",
      "Current iteration=4300, loss=0.24735853146689443\n",
      "Current iteration=4400, loss=0.24735316113439093\n",
      "Current iteration=4500, loss=0.24734805432972964\n",
      "Current iteration=4600, loss=0.24734319306114333\n",
      "Current iteration=4700, loss=0.24733856073183305\n",
      "Current iteration=4800, loss=0.24733414201792614\n",
      "Current iteration=4900, loss=0.24732992275864035\n",
      "Current iteration=5000, loss=0.24732588985725856\n",
      "Current iteration=5100, loss=0.24732203119170146\n",
      "Current iteration=5200, loss=0.24731833553363486\n",
      "Current iteration=5300, loss=0.24731479247518043\n",
      "Current iteration=5400, loss=0.24731139236241814\n",
      "Current iteration=5500, loss=0.24730812623495993\n",
      "Current iteration=5600, loss=0.2473049857709638\n",
      "Current iteration=5700, loss=0.24730196323703224\n",
      "Current iteration=5800, loss=0.24729905144250072\n",
      "Current iteration=5900, loss=0.24729624369768213\n",
      "Current iteration=6000, loss=0.2472935337756758\n",
      "Current iteration=6100, loss=0.24729091587740135\n",
      "Current iteration=6200, loss=0.247288384599548\n",
      "Current iteration=6300, loss=0.24728593490516684\n",
      "Current iteration=6400, loss=0.24728356209666028\n",
      "Current iteration=6500, loss=0.2472812617909523\n",
      "Current iteration=6600, loss=0.24727902989663958\n",
      "Current iteration=6700, loss=0.24727686259295104\n",
      "Current iteration=6800, loss=0.24727475631035387\n",
      "Current iteration=6900, loss=0.24727270771266674\n",
      "Current iteration=7000, loss=0.2472707136805458\n",
      "Current iteration=7100, loss=0.24726877129623362\n",
      "Current iteration=7200, loss=0.24726687782945977\n",
      "Current iteration=7300, loss=0.24726503072440073\n",
      "Current iteration=7400, loss=0.2472632275876121\n",
      "Current iteration=7500, loss=0.24726146617685243\n",
      "Current iteration=7600, loss=0.24725974439072865\n",
      "Current iteration=7700, loss=0.24725806025909702\n",
      "Current iteration=7800, loss=0.24725641193416148\n",
      "Current iteration=7900, loss=0.24725479768221212\n",
      "Current iteration=8000, loss=0.24725321587595617\n",
      "Current iteration=8100, loss=0.2472516649873968\n",
      "Current iteration=8200, loss=0.2472501435812144\n",
      "Current iteration=8300, loss=0.24724865030861648\n",
      "Current iteration=8400, loss=0.24724718390161832\n",
      "Current iteration=8500, loss=0.24724574316772427\n",
      "Current iteration=8600, loss=0.24724432698497606\n",
      "Current iteration=8700, loss=0.24724293429734615\n",
      "Current iteration=8800, loss=0.2472415641104488\n",
      "Current iteration=8900, loss=0.24724021548754277\n",
      "Current iteration=9000, loss=0.24723888754581047\n",
      "Current iteration=9100, loss=0.24723757945288857\n",
      "Current iteration=9200, loss=0.24723629042363368\n",
      "Current iteration=9300, loss=0.24723501971710904\n",
      "Current iteration=9400, loss=0.24723376663377072\n",
      "Current iteration=9500, loss=0.24723253051284577\n",
      "Current iteration=9600, loss=0.24723131072988397\n",
      "Current iteration=9700, loss=0.24723010669447365\n",
      "Current iteration=9800, loss=0.24722891784810955\n",
      "Current iteration=9900, loss=0.24722774366220016\n",
      "loss=0.25668539923809364\n"
     ]
    }
   ],
   "source": [
    "w,loss= logistic_regression(y_tr, tx_tr, initial_w, max_iters, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6046881574088626\n",
      "Current iteration=100, loss=0.25339777445437145\n",
      "Current iteration=200, loss=0.2511641976501825\n",
      "Current iteration=300, loss=0.2505238156845519\n",
      "Current iteration=400, loss=0.2502513434531619\n",
      "Current iteration=500, loss=0.2501033920836258\n",
      "Current iteration=600, loss=0.2500052502528166\n",
      "Current iteration=700, loss=0.24993110482341782\n",
      "Current iteration=800, loss=0.24987105980806934\n",
      "Current iteration=900, loss=0.24982068041987632\n",
      "Current iteration=1000, loss=0.2497775940874236\n",
      "Current iteration=1100, loss=0.24974032006262192\n",
      "Current iteration=1200, loss=0.24970782570367855\n",
      "Current iteration=1300, loss=0.2496793372745955\n",
      "Current iteration=1400, loss=0.24965424855203278\n",
      "Current iteration=1500, loss=0.24963207100708112\n",
      "Current iteration=1600, loss=0.24961240358171263\n",
      "Current iteration=1700, loss=0.24959491269844467\n",
      "Current iteration=1800, loss=0.2495793181601774\n",
      "Current iteration=1900, loss=0.24956538274302884\n",
      "Current iteration=2000, loss=0.24955290427002977\n",
      "Current iteration=2100, loss=0.24954170944000748\n",
      "Current iteration=2200, loss=0.2495316489452862\n",
      "Current iteration=2300, loss=0.24952259356103598\n",
      "Current iteration=2400, loss=0.24951443098123854\n",
      "Current iteration=2500, loss=0.24950706323664906\n",
      "Current iteration=2600, loss=0.24950040457162528\n",
      "Current iteration=2700, loss=0.24949437968622265\n",
      "Current iteration=2800, loss=0.2494889222714819\n",
      "Current iteration=2900, loss=0.2494839737818452\n",
      "Current iteration=3000, loss=0.24947948240070045\n",
      "Current iteration=3100, loss=0.2494754021642402\n",
      "Current iteration=3200, loss=0.24947169221590124\n",
      "Current iteration=3300, loss=0.2494683161691285\n",
      "Current iteration=3400, loss=0.24946524156049446\n",
      "Current iteration=3500, loss=0.24946243937857202\n",
      "Current iteration=3600, loss=0.24945988365662722\n",
      "Current iteration=3700, loss=0.2494575511193273\n",
      "Current iteration=3800, loss=0.2494554208753526\n",
      "Current iteration=3900, loss=0.24945347414919003\n",
      "Current iteration=4000, loss=0.24945169404648151\n",
      "Current iteration=4100, loss=0.2494500653482205\n",
      "Current iteration=4200, loss=0.2494485743298306\n",
      "Current iteration=4300, loss=0.24944720860176736\n",
      "Current iteration=4400, loss=0.24944595696879868\n",
      "Current iteration=4500, loss=0.24944480930553234\n",
      "Current iteration=4600, loss=0.24944375644612193\n",
      "Current iteration=4700, loss=0.24944279008636228\n",
      "Current iteration=4800, loss=0.24944190269664832\n",
      "Current iteration=4900, loss=0.24944108744447402\n",
      "Current iteration=5000, loss=0.2494403381253219\n",
      "Current iteration=5100, loss=0.24943964910095837\n",
      "Current iteration=5200, loss=0.24943901524426246\n",
      "Current iteration=5300, loss=0.24943843188984016\n",
      "Current iteration=5400, loss=0.24943789478976433\n",
      "Current iteration=5500, loss=0.24943740007386456\n",
      "Current iteration=5600, loss=0.24943694421406015\n",
      "Current iteration=5700, loss=0.24943652399229252\n",
      "Current iteration=5800, loss=0.24943613647166726\n",
      "Current iteration=5900, loss=0.2494357789704568\n",
      "Current iteration=6000, loss=0.24943544903866302\n",
      "Current iteration=6100, loss=0.24943514443686757\n",
      "Current iteration=6200, loss=0.2494348631171311\n",
      "Current iteration=6300, loss=0.2494346032057285\n",
      "Current iteration=6400, loss=0.2494343629875355\n",
      "Current iteration=6500, loss=0.24943414089189192\n",
      "Current iteration=6600, loss=0.2494339354798002\n",
      "Current iteration=6700, loss=0.2494337454323181\n",
      "Current iteration=6800, loss=0.24943356954003343\n",
      "Current iteration=6900, loss=0.24943340669351025\n",
      "Current iteration=7000, loss=0.24943325587461143\n",
      "Current iteration=7100, loss=0.24943311614861535\n",
      "Current iteration=7200, loss=0.2494329866570477\n",
      "Current iteration=7300, loss=0.24943286661115896\n",
      "Current iteration=7400, loss=0.24943275528598935\n",
      "Current iteration=7500, loss=0.2494326520149638\n",
      "Current iteration=7600, loss=0.24943255618496543\n",
      "Current iteration=7700, loss=0.2494324672318465\n",
      "Current iteration=7800, loss=0.24943238463633308\n",
      "Current iteration=7900, loss=0.24943230792028906\n",
      "Current iteration=8000, loss=0.24943223664330452\n",
      "Current iteration=8100, loss=0.24943217039958154\n",
      "Current iteration=8200, loss=0.2494321088150866\n",
      "Current iteration=8300, loss=0.24943205154495102\n",
      "Current iteration=8400, loss=0.2494319982710909\n",
      "Current iteration=8500, loss=0.24943194870003052\n",
      "Current iteration=8600, loss=0.24943190256091186\n",
      "Current iteration=8700, loss=0.24943185960367042\n",
      "Current iteration=8800, loss=0.24943181959736568\n",
      "Current iteration=8900, loss=0.24943178232865157\n",
      "Current iteration=9000, loss=0.2494317476003735\n",
      "Current iteration=9100, loss=0.24943171523028365\n",
      "Current iteration=9200, loss=0.2494316850498612\n",
      "Current iteration=9300, loss=0.2494316569032303\n",
      "Current iteration=9400, loss=0.24943163064616852\n",
      "Current iteration=9500, loss=0.2494316061451936\n",
      "Current iteration=9600, loss=0.24943158327672726\n",
      "Current iteration=9700, loss=0.24943156192632632\n",
      "Current iteration=9800, loss=0.24943154198797432\n",
      "Current iteration=9900, loss=0.24943152336343435\n",
      "loss=0.2547312137880527\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.0005\n",
    "w_reg,loss_reg= reg_logistic_regression(y_tr, tx_tr, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277, 1)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_reg.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_poly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m build_poly(x_train, \u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_poly' is not defined"
     ]
    }
   ],
   "source": [
    "build_poly(x_train, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tx_te=np.c_[np.ones((y_te.shape[0], 1)), x_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65627, 277)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277, 1)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te=np.where(y_te==-1, 0, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=apply_model(tx_te, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate accuracy\n",
    "correct_predictions = np.sum(y_pred == y_te)\n",
    "total_samples = len(y_te)\n",
    "accuracy = correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3508144616607072"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1_score(y_te, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8506102671156689\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(test, model):\n",
    "    pred=(sigmoid(test.dot(model))>=0.25).astype(int)\n",
    "    return(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt=replace_nan_by_mean(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109379, 321)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_filtered=filtering(xt, test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109379, 75)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmented_data_test=feature_expansion(xt_filtered, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmented_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xt_standardized=standardize(xt_filtered)\n",
    "xtest=np.c_[np.ones((xt.shape[0], 1)), xt_standardized]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109379, 76)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=apply_model(xtest, w)\n",
    "predictions=np.where(predictions==0,-1, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1],\n",
       "       [-1],\n",
       "       [-1],\n",
       "       ...,\n",
       "       [-1],\n",
       "       [-1],\n",
       "       [-1]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, predictions, 'predictions_LR_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
