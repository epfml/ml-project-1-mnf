{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from helpers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset_to_release/'\n",
    "train_data_path = \"./dataset_to_release/x_train.csv\"\n",
    "test_data_path = \"./dataset_to_release/x_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data_all(data_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "\n",
    "def replace_nan_by_mean(data):\n",
    "    ''' function that handels the missing values by replacing them with the column means'''\n",
    "    nan_indices = np.isnan(data)\n",
    "    column_means = np.nanmean(data, axis=0)\n",
    "    data[nan_indices] = np.take(column_means, np.where(nan_indices)[1])\n",
    "    return data\n",
    "\n",
    "data_train = replace_nan_by_mean(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 321)\n",
      "(328135, 321)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data filtering: we only keep relevant features\n",
    "\n",
    "def filtering(data, data_path):\n",
    "    columns = extract_first_line(data_path).split(',')\n",
    "    columns.pop(0)\n",
    "    columns_to_keep = []\n",
    "    for c in columns:\n",
    "        if c.startswith('_'):\n",
    "            columns_to_keep.append(c)\n",
    "    indices_to_keep = [columns.index(c) for c in columns_to_keep]\n",
    "    data_f = data[:, indices_to_keep]\n",
    "    return(data_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = [\"_AGE80\", \"_AGE65YR\", \"_AGEG5YR\", \"_AGE_G\", \"_AIDTST3\", \"_ASTHMS1\", \"_BMI5\", \"_BMI5CAT\",\n",
    "                     \"_CASTHM1\", \"_CHLDCNT\", \"_CHOLCHK\", \"_DRDXAR1\", \"_DRNKWEK\", \"_DUALCOR\", \"_DUALUSE\"\n",
    "                     , \"_FLSHOT6\", \"_FRT16\", \"_FRTLT1\", \"_FRTRESP\", \"_FRUITEX\", \"_FRUTSUM\", \"_HCVU651\",\n",
    "                       \"_LLCPWT\", \"_LMTACT1\", \"_LMTSCL1\", \"_LMTWRK1\", \"_LTASTH1\", \"_MICHD\", \"_MINAC11\", \"_MINAC21\", \n",
    "                       \"_MISFRTN\", \"_MISVEGN\", \"_MRACE1\", \"_PA30021\", \"_PA150R2\", \"_PA300R2\", \"_PACAT1\", \"_PAINDX1\", \n",
    "                       \"_PASTAE1\", \"_PASTRNG\", \"_PNEUMO2\",\n",
    "                       \"_RFBING5\", \"_RFBMI5\", \"_RFCHOL\", \"_RFDRHV5\", \"_RFHLTH\", \"_RFHYPE5\", \"_RFSMOK3\", \n",
    "                       \"_SMOKER3\", \"_TOTINDA\", \"_VEG23\", \"_VEGESUM\", \"_VEGETEX\", \"_VEGLT1\", \"_VEGRESP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(328135, 54)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second version of data filtering, remove 9 more columns\n",
    "\n",
    "def filtering_2(data,data_path):\n",
    "    columns = extract_first_line(data_path).split(',')\n",
    "    columns.pop(0)\n",
    "    filtered_columns = [col for col in columns if col in features_to_keep]\n",
    "    indices_to_keep = [columns.index(c) for c in filtered_columns]\n",
    "    print(len(indices_to_keep))\n",
    "    data_f = data[:, indices_to_keep]\n",
    "    return(data_f)\n",
    "\n",
    "data_train_filtered = filtering_2(data_train, train_data_path)\n",
    "data_train_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization of the data\n",
    "def standardize(data):\n",
    "    small_value = 1*10**(-9)\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0) + small_value\n",
    "    return((data - mean) / (std))\n",
    "\n",
    "data_train_standard = standardize(data_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature augmentation\n",
    "def feature_expansion(data, degree):\n",
    "    augmented_features = []\n",
    "    for i in range(data.shape[1]):\n",
    "        feature = data[:,i]\n",
    "        augmented_feature = build_poly(feature, degree)\n",
    "        augmented_features.append(augmented_feature)\n",
    "\n",
    "    # Stack the augmented features horizontally\n",
    "    augmented_data = np.hstack(augmented_features)\n",
    "    return(augmented_data)\n",
    "\n",
    "augmented_data = feature_expansion(data_train_standard, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Computes the F1 score for a classification model using NumPy.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels (numpy.ndarray): True labels for the data.\n",
    "    predicted_labels (numpy.ndarray): Predicted labels from the model.\n",
    "\n",
    "    Returns:\n",
    "    f1 (float): The F1 score.\n",
    "    \"\"\"\n",
    "    true_positive = np.sum(np.logical_and(true_labels == 1, predicted_labels == 1))\n",
    "    false_positive = np.sum(np.logical_and(true_labels == 0, predicted_labels == 1))\n",
    "    false_negative = np.sum(np.logical_and(true_labels == 1, predicted_labels == 0))\n",
    "    \n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "\n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)]\n",
    "    \n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(test, model):\n",
    "    pred = (sigmoid(test.dot(model))>=0.35).astype(int)\n",
    "    return(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\n",
    "\n",
    "    Args:\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        degree:     scalar, cf. build_poly()\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # get k'th subgroup in test, others in train:\n",
    "    train_idx = np.reshape(k_indices[[i for i in range(len(k_indices)) if i!=k]], -1)\n",
    "    test_idx = k_indices[k]\n",
    "\n",
    "    x_train = x[train_idx,:]\n",
    "    y_train = y[train_idx]\n",
    "    x_test = x[test_idx,:]\n",
    "    y_test = y[test_idx]\n",
    "    \n",
    "    y_tr = np.expand_dims(y_train, 1)\n",
    "    y_te = np.expand_dims(y_test, 1)\n",
    "\n",
    "    y_tr = np.where(y_tr == -1, 0, y_tr)\n",
    "    y_te = np.where(y_te == -1, 0, y_te)\n",
    "\n",
    "    max_iters = 1000\n",
    "    gamma = 0.5\n",
    "\n",
    "    # form data with polynomial degree:\n",
    "    train_data = feature_expansion(x_train, degree)\n",
    "    test_data = feature_expansion(x_test, degree)\n",
    "    train_data = standardize(train_data)\n",
    "    test_data = standardize(test_data)\n",
    "    \n",
    "    # build tx\n",
    "    tx_tr = np.c_[np.ones((y_train.shape[0], 1)), train_data]\n",
    "    tx_te = np.c_[np.ones((test_data.shape[0], 1)), test_data]\n",
    "    initial_w = np.zeros((tx_tr.shape[1], 1))\n",
    "\n",
    "    # reg logistic regression: \n",
    "    w = reg_logistic_regression(y_tr,tx_tr,lambda_,initial_w, max_iters, gamma)[0]\n",
    "    y_pred = apply_model(tx_te, w)\n",
    "    \n",
    "    # calculate f1 score on test:\n",
    "    f1_te = compute_f1_score(y_te, y_pred)\n",
    "  \n",
    "    return f1_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_demo(degree, k_fold, lambdas):\n",
    "    \"\"\"cross validation over regularisation parameter lambda.\n",
    "\n",
    "    Args:\n",
    "        degree: integer, degree of the polynomial expansion\n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : scalar, the associated root mean squared error for the best lambda\n",
    "    \"\"\"\n",
    "\n",
    "    seed = 12\n",
    "    #degree = degree\n",
    "    k_fold = k_fold\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    f1_score=np.zeros((len(degree), len(lambdas)))\n",
    "    # cross validation over lambdas:\n",
    "    for i in range(len(degree)):\n",
    "        d=degree[i]\n",
    "        for j in range(len(lambdas)):\n",
    "            lambda_ = lambdas[j]\n",
    "            cross_val = [cross_validation(y_train, data_train_standard, k_indices, k, lambda_, d) for k in range(k_fold)]\n",
    "            f1 = np.mean(cross_val)\n",
    "            f1_score[i,j] = f1\n",
    "    best_degree = degree[np.unravel_index(np.argmax(f1_score, axis=None), f1_score.shape)[0]]\n",
    "    best_lambda = lambdas[np.unravel_index(np.argmax(f1_score, axis=None), f1_score.shape)[1]]\n",
    "    best_f1 = np.max(f1_score)\n",
    "    \n",
    "    return best_degree, best_f1 , best_lambda , f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6043003767871573\n",
      "Current iteration=100, loss=0.24195432849718485\n",
      "Current iteration=200, loss=0.23959155567457874\n",
      "Current iteration=300, loss=0.2390008625142806\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_degree, best_f1 , best_lambda , f1_score \u001b[38;5;241m=\u001b[39m cross_validation_demo(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m4\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), \u001b[38;5;241m2\u001b[39m, np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[48], line 25\u001b[0m, in \u001b[0;36mcross_validation_demo\u001b[1;34m(degree, k_fold, lambdas)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lambdas)):\n\u001b[0;32m     24\u001b[0m     lambda_ \u001b[38;5;241m=\u001b[39m lambdas[j]\n\u001b[1;32m---> 25\u001b[0m     cross_val \u001b[38;5;241m=\u001b[39m [cross_validation(y_train, data_train_standard, k_indices, k, lambda_, d) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k_fold)]\n\u001b[0;32m     26\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(cross_val)\n\u001b[0;32m     27\u001b[0m     f1_score[i,j] \u001b[38;5;241m=\u001b[39m f1\n",
      "Cell \u001b[1;32mIn[48], line 25\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lambdas)):\n\u001b[0;32m     24\u001b[0m     lambda_ \u001b[38;5;241m=\u001b[39m lambdas[j]\n\u001b[1;32m---> 25\u001b[0m     cross_val \u001b[38;5;241m=\u001b[39m [cross_validation(y_train, data_train_standard, k_indices, k, lambda_, d) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k_fold)]\n\u001b[0;32m     26\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(cross_val)\n\u001b[0;32m     27\u001b[0m     f1_score[i,j] \u001b[38;5;241m=\u001b[39m f1\n",
      "Cell \u001b[1;32mIn[47], line 47\u001b[0m, in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, lambda_, degree)\u001b[0m\n\u001b[0;32m     44\u001b[0m initial_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((tx_tr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# reg logistic regression: \u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m w \u001b[38;5;241m=\u001b[39m reg_logistic_regression(y_tr,tx_tr,lambda_,initial_w, max_iters, gamma)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     48\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m apply_model(tx_te, w)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# calculate f1 score on test:\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\ml-project-1-mnf\\implementations.py:286\u001b[0m, in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m    284\u001b[0m grad\u001b[38;5;241m=\u001b[39mcalculate_log_likelihood_gradient(y,tx,w) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mlambda_\u001b[38;5;241m*\u001b[39mw\n\u001b[0;32m    285\u001b[0m w\u001b[38;5;241m=\u001b[39mw\u001b[38;5;241m-\u001b[39mgamma\u001b[38;5;241m*\u001b[39mgrad\n\u001b[1;32m--> 286\u001b[0m loss\u001b[38;5;241m=\u001b[39mcalculate_log_likelihood_loss(y,tx,w)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# log info\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\ml-project-1-mnf\\implementations.py:187\u001b[0m, in \u001b[0;36mcalculate_log_likelihood_loss\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    185\u001b[0m N\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    186\u001b[0m y_pred\u001b[38;5;241m=\u001b[39mtx\u001b[38;5;241m.\u001b[39mdot(w)\n\u001b[1;32m--> 187\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean((y\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(sigmoid(y_pred))\u001b[38;5;241m+\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msigmoid(y_pred))))\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_degree, best_f1 , best_lambda , f1_score = cross_validation_demo(np.array([4]).astype(int), 2, np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best_degree is 4, the best lambda is 0.01 and the best f1 is 0.3240984911927453:\n"
     ]
    }
   ],
   "source": [
    "print(\"The best_degree is {}, the best lambda is {} and the best f1 is {}:\".format(best_degree, best_lambda, best_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135, 270)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standardized = standardize(augmented_data)\n",
    "data_standardized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test set in two\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8\n",
    "    you will have 80% of your data set dedicated to training\n",
    "    and the rest dedicated to testing. If ratio times the number of samples is not round\n",
    "    you can use np.floor. Also check the documentation for np.random.permutation,\n",
    "    it could be useful.\n",
    "\n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        y: numpy array of shape (N,).\n",
    "        ratio: scalar in [0,1]\n",
    "        seed: integer.\n",
    "\n",
    "    Returns:\n",
    "        x_tr: numpy array containing the train data.\n",
    "        x_te: numpy array containing the test data.\n",
    "        y_tr: numpy array containing the train labels.\n",
    "        y_te: numpy array containing the test labels.\n",
    "    \"\"\"\n",
    "    N=int(ratio*len(x))\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # split the data based on the given ratio: \n",
    "    shuffled_data = np.random.permutation(x)\n",
    "    shuffled_labels = np.random.permutation(y)\n",
    "    np.random.seed(seed)\n",
    "    x_tr = shuffled_data[:N] #train data\n",
    "    x_te = shuffled_data[N:] #test data\n",
    "    y_tr = shuffled_labels[:N] #train labels\n",
    "    y_te = shuffled_labels[N:] # test labels\n",
    "\n",
    "    return(x_tr,x_te, y_tr, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr,x_te, y_tr, y_te = split_data(data_standardized, y_train, ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification using logistic regression\n",
    "\n",
    "max_iters = 2000\n",
    "gamma = 0.5\n",
    "\n",
    " # Build tx\n",
    "tx_tr = np.c_[np.ones((y_train.shape[0], 1)), data_standardized]\n",
    "initial_w = np.zeros((tx_tr.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.expand_dims(y_train, 1)\n",
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad = calculate_log_likelihood_gradient(y_train,tx_tr,initial_w)\n",
    "# loss = calculate_log_likelihood_loss(y_train,tx_tr,initial_w)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6043822619210014\n",
      "Current iteration=100, loss=0.24285325687469855\n",
      "Current iteration=200, loss=0.24048368896360986\n",
      "Current iteration=300, loss=0.2398927053722558\n",
      "Current iteration=400, loss=0.23961441616080237\n",
      "Current iteration=500, loss=0.2394396533106269\n",
      "Current iteration=600, loss=0.2393145463037291\n",
      "Current iteration=700, loss=0.23921886153579436\n",
      "Current iteration=800, loss=0.23914283264365963\n",
      "Current iteration=900, loss=0.23908093361352936\n",
      "Current iteration=1000, loss=0.23902968961132795\n",
      "Current iteration=1100, loss=0.23898674532729466\n",
      "Current iteration=1200, loss=0.23895041462607067\n",
      "Current iteration=1300, loss=0.2389194415062791\n",
      "Current iteration=1400, loss=0.2388928628747833\n",
      "Current iteration=1500, loss=0.23886992419503963\n",
      "Current iteration=1600, loss=0.2388500245489164\n",
      "Current iteration=1700, loss=0.23883267914039452\n",
      "Current iteration=1800, loss=0.23881749274294256\n",
      "Current iteration=1900, loss=0.23880414034521819\n",
      "Current iteration=2000, loss=0.23879235271515317\n",
      "Current iteration=2100, loss=0.23878190542820218\n",
      "Current iteration=2200, loss=0.23877261039609035\n",
      "Current iteration=2300, loss=0.2387643092375428\n",
      "Current iteration=2400, loss=0.23875686802962462\n",
      "Current iteration=2500, loss=0.23875017310964744\n",
      "Current iteration=2600, loss=0.23874412768727943\n",
      "Current iteration=2700, loss=0.23873864908901885\n",
      "Current iteration=2800, loss=0.23873366650153965\n",
      "Current iteration=2900, loss=0.2387291191123976\n",
      "Current iteration=3000, loss=0.2387249545699606\n",
      "Current iteration=3100, loss=0.23872112770175455\n",
      "Current iteration=3200, loss=0.23871759944342621\n",
      "Current iteration=3300, loss=0.23871433594039076\n",
      "Current iteration=3400, loss=0.238711307791833\n",
      "Current iteration=3500, loss=0.2387084894126003\n",
      "Current iteration=3600, loss=0.23870585849315337\n",
      "Current iteration=3700, loss=0.23870339554137554\n",
      "Current iteration=3800, loss=0.23870108349295174\n",
      "Current iteration=3900, loss=0.23869890737935784\n",
      "Current iteration=4000, loss=0.23869685404438737\n",
      "Current iteration=4100, loss=0.23869491190167066\n",
      "Current iteration=4200, loss=0.2386930707268974\n",
      "Current iteration=4300, loss=0.23869132147947614\n",
      "Current iteration=4400, loss=0.23868965614921675\n",
      "Current iteration=4500, loss=0.23868806762431902\n",
      "Current iteration=4600, loss=0.23868654957753638\n",
      "Current iteration=4700, loss=0.23868509636786794\n",
      "Current iteration=4800, loss=0.23868370295553787\n",
      "Current iteration=4900, loss=0.23868236482836377\n",
      "Current iteration=5000, loss=0.2386810779378959\n",
      "Current iteration=5100, loss=0.23867983864395736\n",
      "Current iteration=5200, loss=0.23867864366641015\n",
      "Current iteration=5300, loss=0.23867749004315056\n",
      "Current iteration=5400, loss=0.23867637509347736\n",
      "Current iteration=5500, loss=0.23867529638610224\n",
      "Current iteration=5600, loss=0.2386742517111751\n",
      "Current iteration=5700, loss=0.23867323905578744\n",
      "Current iteration=5800, loss=0.23867225658249014\n",
      "Current iteration=5900, loss=0.23867130261042893\n",
      "Current iteration=6000, loss=0.23867037559875534\n",
      "Current iteration=6100, loss=0.2386694741320181\n",
      "Current iteration=6200, loss=0.23866859690727854\n",
      "Current iteration=6300, loss=0.2386677427227327\n",
      "Current iteration=6400, loss=0.23866691046764701\n",
      "Current iteration=6500, loss=0.23866609911344466\n",
      "Current iteration=6600, loss=0.23866530770579727\n",
      "Current iteration=6700, loss=0.23866453535760057\n",
      "Current iteration=6800, loss=0.2386637812427243\n",
      "Current iteration=6900, loss=0.23866304459044307\n",
      "Current iteration=7000, loss=0.23866232468046597\n",
      "Current iteration=7100, loss=0.23866162083849515\n",
      "Current iteration=7200, loss=0.23866093243224987\n",
      "Current iteration=7300, loss=0.23866025886790118\n",
      "Current iteration=7400, loss=0.23865959958687294\n",
      "Current iteration=7500, loss=0.2386589540629622\n",
      "Current iteration=7600, loss=0.23865832179974814\n",
      "Current iteration=7700, loss=0.23865770232825392\n",
      "Current iteration=7800, loss=0.2386570952048342\n",
      "Current iteration=7900, loss=0.23865650000926408\n",
      "Current iteration=8000, loss=0.23865591634300704\n",
      "Current iteration=8100, loss=0.23865534382764367\n",
      "Current iteration=8200, loss=0.23865478210344193\n",
      "Current iteration=8300, loss=0.23865423082805756\n",
      "Current iteration=8400, loss=0.2386536896753479\n",
      "Current iteration=8500, loss=0.23865315833428855\n",
      "Current iteration=8600, loss=0.23865263650798407\n",
      "Current iteration=8700, loss=0.23865212391276097\n",
      "Current iteration=8800, loss=0.23865162027733558\n",
      "Current iteration=8900, loss=0.23865112534205063\n",
      "Current iteration=9000, loss=0.23865063885817298\n",
      "Current iteration=9100, loss=0.2386501605872455\n",
      "Current iteration=9200, loss=0.2386496903004907\n",
      "Current iteration=9300, loss=0.2386492277782596\n",
      "Current iteration=9400, loss=0.23864877280952082\n",
      "Current iteration=9500, loss=0.2386483251913886\n",
      "Current iteration=9600, loss=0.238647884728685\n",
      "Current iteration=9700, loss=0.23864745123353232\n",
      "Current iteration=9800, loss=0.2386470245249748\n",
      "Current iteration=9900, loss=0.23864660442862665\n",
      "loss=0.24017046965143968\n"
     ]
    }
   ],
   "source": [
    "w,loss = logistic_regression(y_train, tx_tr, initial_w, max_iters, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_ = 10e-4\n",
    "# w_reg,loss_reg = reg_logistic_regression(y_train, tx_tr, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65627, 270)\n",
      "(271, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_te.shape)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tx_te' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m y_te \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_te \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, y_te)\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m apply_model(tx_te, w)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tx_te' is not defined"
     ]
    }
   ],
   "source": [
    "y_te = np.where(y_te ==-1, 0, y_te)\n",
    "y_pred = apply_model(tx_te, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate accuracy\n",
    "correct_predictions = np.sum(y_pred == y_te)\n",
    "total_samples = len(y_te)\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3508144616607072"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1_score(y_te, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = replace_nan_by_mean(x_test)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "xt_filtered = filtering_2(xt, test_data_path)\n",
    "print(xt_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_expansion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m augmented_data_test\u001b[38;5;241m=\u001b[39mfeature_expansion(xt_filtered, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(augmented_data_test\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_expansion' is not defined"
     ]
    }
   ],
   "source": [
    "augmented_data_test = feature_expansion(xt_filtered, 5)\n",
    "print(augmented_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'standardize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m xt_standardized\u001b[38;5;241m=\u001b[39mstandardize(augmented_data_test)\n\u001b[0;32m      2\u001b[0m xtest\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mc_[np\u001b[38;5;241m.\u001b[39mones((xt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)), xt_standardized]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(xtest\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'standardize' is not defined"
     ]
    }
   ],
   "source": [
    "xt_standardized = standardize(augmented_data_test)\n",
    "xtest = np.c_[np.ones((xt.shape[0], 1)), xt_standardized]\n",
    "print(xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = apply_model(xtest, w_reg)\n",
    "predictions = np.where(predictions == 0,-1, predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, predictions, 'predictions_LR_0.2_5.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
