{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from helpers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset_to_release/'\n",
    "train_data_path = \"./dataset_to_release/x_train.csv\"\n",
    "test_data_path = \"./dataset_to_release/x_test.csv\"\n",
    "\n",
    "x_train, x_test, y_train, train_ids, test_ids =load_csv_data_all(data_path, sub_sample=False)\n",
    "\n",
    "# Change the label -1 by 0\n",
    "y_train = np.where(y_train == -1, 0, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "\n",
    "def replace_nan_by_mean(data):\n",
    "    ''' function that handels the missing values by replacing them with the column means'''\n",
    "    nan_indices = np.isnan(data)\n",
    "    column_means = np.nanmean(data, axis=0)\n",
    "    data[nan_indices] = np.take(column_means, np.where(nan_indices)[1])\n",
    "    return data\n",
    "\n",
    "data_train = replace_nan_by_mean(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data filtering\n",
    "\n",
    "features_to_keep = [\"_AGE80\", \"_AGE65YR\", \"_AGEG5YR\", \"_AGE_G\", \"_AIDTST3\", \"_ASTHMS1\", \"_BMI5\", \"_BMI5CAT\",\n",
    "                    \"_CASTHM1\", \"_CHLDCNT\", \"_CHOLCHK\", \"_DRDXAR1\", \"_DRNKWEK\", \"_FLSHOT6\", \"_FRT16\", \n",
    "                    \"_FRTLT1\", \"_FRTRESP\", \"_FRUITEX\", \"_FRUTSUM\", \"_HCVU651\", \"_LMTACT1\", \"_LMTSCL1\", \n",
    "                    \"_LMTWRK1\", \"_LTASTH1\", \"_MINAC11\", \"_MINAC21\", \"_MISFRTN\", \"_MISVEGN\", \"_PA30021\", \n",
    "                    \"_PA150R2\", \"_PACAT1\", \"_PAINDX1\", \"_PASTAE1\", \"_PASTRNG\", \"_PNEUMO2\", \"_RFBING5\", \n",
    "                    \"_RFBMI5\", \"_RFCHOL\", \"_RFDRHV5\", \"_RFHLTH\", \"_RFHYPE5\", \"_RFSMOK3\", \"_SMOKER3\", \n",
    "                    \"_TOTINDA\", \"_VEG23\", \"_VEGESUM\", \"_VEGETEX\", \"_VEGLT1\", \"_VEGRESP\"]\n",
    "\n",
    "def filtering(data,data_path):\n",
    "    columns = extract_first_line(data_path).split(',')\n",
    "    columns.pop(0)\n",
    "    filtered_columns = [col for col in columns if col in features_to_keep]\n",
    "    indices_to_keep = [columns.index(c) for c in filtered_columns]\n",
    "    data_f = data[:, indices_to_keep]\n",
    "    return(data_f)\n",
    "\n",
    "data_train_filtered=filtering(data_train, train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization of the data\n",
    "def standardize(data):\n",
    "    small_value = 1*10**(-9)\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)+small_value\n",
    "    return((data - mean) / (std))\n",
    "\n",
    "data_train_standard = standardize(data_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature augmentation\n",
    "def feature_expansion(data, degree):\n",
    "    augmented_features = []\n",
    "    for i in range(data.shape[1]):\n",
    "        feature = data[:,i]\n",
    "        augmented_feature = build_poly(feature, degree)\n",
    "        augmented_features.append(augmented_feature)\n",
    "\n",
    "    # Stack the augmented features horizontally\n",
    "    augmented_data = np.hstack(augmented_features)\n",
    "    return(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(test, model):\n",
    "    pred = (sigmoid(test.dot(model))>=0.14).astype(int)\n",
    "    return(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling to balance the data \n",
    "\n",
    "def undersample(X, y, class_):\n",
    "    \"\"\"\n",
    "    Undersample two arrays X and Y according to a class 'class_'.\n",
    "\n",
    "    Parameters:\n",
    "    X (array) : array of training data\n",
    "    Y (array) : array of training data\n",
    "\n",
    "    Returns:\n",
    "    X_undersampled (array): Undersampled array of the concerned class\n",
    "    Y_undersampled (array): Undersampled array of the concerned class\n",
    "    \"\"\"\n",
    "    indices = np.where(y == class_)[0]\n",
    "    no_indices = np.where(y != class_)[0]\n",
    "    number_no_indices = len(no_indices)\n",
    "    undersample_indices = np.random.choice(indices, number_no_indices, replace=False)\n",
    "    \n",
    "    keep_indices = np.concatenate([undersample_indices, no_indices])\n",
    "    X_undersampled = X[keep_indices]\n",
    "    y_undersampled = y[keep_indices]\n",
    "    \n",
    "    return X_undersampled, y_undersampled\n",
    "\n",
    "x_undersampled, y_undersampled =undersample(data_train_standard, y_train, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Computes the F1 score for a classification model using NumPy.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels (numpy.ndarray): True labels for the data.\n",
    "    predicted_labels (numpy.ndarray): Predicted labels from the model.\n",
    "\n",
    "    Returns:\n",
    "    f1 (float): The F1 score.\n",
    "    \"\"\"\n",
    "    true_positive = np.sum(np.logical_and(true_labels == 1, predicted_labels == 1))\n",
    "    false_positive = np.sum(np.logical_and(true_labels == 0, predicted_labels == 1))\n",
    "    false_negative = np.sum(np.logical_and(true_labels == 1, predicted_labels == 0))\n",
    "    \n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Compute the accuracy of predictions\n",
    "\n",
    "def compute_accuracy(pred, label):\n",
    "    correct_predictions = np.sum((pred == label))\n",
    "    total_samples = len(label)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "\n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)]\n",
    "    \n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\n",
    "\n",
    "    Args:\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        degree:     scalar, cf. build_poly()\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # get k'th subgroup in test, others in train:\n",
    "    train_idx = np.reshape(k_indices[[i for i in range(len(k_indices)) if i != k]], -1)\n",
    "    test_idx = k_indices[k]\n",
    "\n",
    "    x_train = x[train_idx, :]\n",
    "    y_train = y[train_idx]\n",
    "    x_test = x[test_idx, :]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    y_tr = np.expand_dims(y_train, 1)\n",
    "    y_te = np.expand_dims(y_test, 1)\n",
    "\n",
    "    y_tr = np.where(y_tr == -1, 0, y_tr)\n",
    "    y_te = np.where(y_te == -1, 0, y_te)\n",
    "\n",
    "    max_iters = 1000\n",
    "    gamma = 0.5\n",
    "\n",
    "    # ***************************************************\n",
    "    # form data with polynomial degree :\n",
    "\n",
    "    train_data = feature_expansion(x_train, degree)\n",
    "    test_data = feature_expansion(x_test, degree)\n",
    "    train_data = standardize(train_data)\n",
    "    test_data = standardize(test_data)\n",
    "    # ***************************************************\n",
    "    # build tx :\n",
    "    tx_tr = np.c_[np.ones((y_train.shape[0], 1)), train_data]\n",
    "    tx_te = np.c_[np.ones((test_data.shape[0], 1)), test_data]\n",
    "    initial_w = np.zeros((tx_tr.shape[1], 1))\n",
    "\n",
    "    # reg logistic regression :\n",
    "    w = reg_logistic_regression(y_tr, tx_tr, lambda_, initial_w, max_iters, gamma)[0]\n",
    "    y_pred = apply_model(tx_te, w)\n",
    "    # calculate f1 score on test :\n",
    "    f1_te = compute_f1_score(y_te, y_pred)\n",
    "\n",
    "    return f1_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_demo(degree, k_fold, lambdas):\n",
    "    \"\"\"cross validation over regularisation parameter lambda.\n",
    "\n",
    "    Args:\n",
    "        degree: integer, degree of the polynomial expansion\n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_lambda: scalar, value of the best lambda\n",
    "        best_rmse: scalar, the associated root mean squared error for the best lambda\n",
    "    \"\"\"\n",
    "\n",
    "    seed = 12\n",
    "    k_fold = k_fold  # Removed the duplicate k_fold assignment\n",
    "    k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "    f1_score = np.zeros((len(degree), len(lambdas)))\n",
    "    \n",
    "    for i in range(len(degree)):\n",
    "        d = degree[i]\n",
    "        for j in range(len(lambdas)):\n",
    "            lambda_ = lambdas[j]\n",
    "            cross_val = [cross_validation(y_train, data_train_filtered_2, k_indices, k, lambda_, d) for k in range(k_fold)]\n",
    "            f1 = np.mean(cross_val)\n",
    "            f1_score[i, j] = f1\n",
    "            \n",
    "    best_degree = degree[np.unravel_index(np.argmax(f1_score, axis=None), f1_score.shape)[0]]\n",
    "    best_lambda = lambdas[np.unravel_index(np.argmax(f1_score, axis=None), f1_score.shape)[1]]\n",
    "    best_f1 = np.max(f1_score)\n",
    "    \n",
    "    return best_degree, best_f1, best_lambda, f1_score\n",
    "\n",
    "# best_degree, best_f1, best_lambda, f1_score = cross_validation_demo(np.array([1]).astype(int), 4, np.array([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    Split the dataset based on the split ratio. If the ratio is 0.8,\n",
    "    you will have 80% of your dataset dedicated to training,\n",
    "    and the rest is dedicated to testing. If the ratio times the number of samples is not a whole number,\n",
    "    you can use np.floor. Also, check the documentation for np.random.permutation;\n",
    "    it could be useful.\n",
    "\n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        y: numpy array of shape (N,).\n",
    "        ratio: scalar in [0,1]\n",
    "        seed: integer.\n",
    "\n",
    "    Returns:\n",
    "        x_tr: numpy array containing the train data.\n",
    "        x_te: numpy array containing the test data.\n",
    "        y_tr: numpy array containing the train labels.\n",
    "        y_te: numpy array containing the test labels.\n",
    "    \"\"\"\n",
    "    N = int(ratio * len(x))\n",
    "    np.random.seed(seed)\n",
    "    shuffled_data = np.random.permutation(x)\n",
    "    np.random.seed(seed)\n",
    "    shuffled_labels = np.random.permutation(y)\n",
    "    x_tr = shuffled_data[:N]  # train data\n",
    "    x_te = shuffled_data[N:]  # test data\n",
    "    y_tr = shuffled_labels[:N]  # train labels\n",
    "    y_te = shuffled_labels[N:]  # test labels\n",
    "\n",
    "    return x_tr, x_te, y_tr, y_te\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(data_train_standard, y_train, ratio=0.8)\n",
    "y_tr = np.expand_dims(y_tr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6035724632042655\n",
      "Current iteration=100, loss=0.25252331440624926\n",
      "Current iteration=200, loss=0.25092710997432416\n",
      "Current iteration=300, loss=0.2506468243763326\n",
      "Current iteration=400, loss=0.25056331447293845\n",
      "Current iteration=500, loss=0.2505249162600111\n",
      "Current iteration=600, loss=0.2504997167097189\n",
      "Current iteration=700, loss=0.25047978984147706\n",
      "Current iteration=800, loss=0.25046284797009216\n",
      "Current iteration=900, loss=0.2504480543493292\n",
      "Current iteration=1000, loss=0.2504349871295834\n",
      "Current iteration=1100, loss=0.2504233678584978\n",
      "Current iteration=1200, loss=0.2504129842565934\n",
      "Current iteration=1300, loss=0.25040366442065337\n",
      "Current iteration=1400, loss=0.2503952656280911\n",
      "Current iteration=1500, loss=0.2503876679328799\n",
      "Current iteration=1600, loss=0.25038076976501\n",
      "Current iteration=1700, loss=0.25037448462413575\n",
      "Current iteration=1800, loss=0.25036873849400937\n",
      "Current iteration=1900, loss=0.2503634677794887\n",
      "Current iteration=2000, loss=0.25035861763957373\n",
      "Current iteration=2100, loss=0.2503541406274286\n",
      "Current iteration=2200, loss=0.25034999557181753\n",
      "Current iteration=2300, loss=0.25034614665049915\n",
      "Current iteration=2400, loss=0.25034256261772864\n",
      "Current iteration=2500, loss=0.250339216156548\n",
      "Current iteration=2600, loss=0.25033608333292395\n",
      "Current iteration=2700, loss=0.250333143133611\n",
      "Current iteration=2800, loss=0.2503303770732965\n",
      "Current iteration=2900, loss=0.2503277688594199\n",
      "Current iteration=3000, loss=0.250325304105267\n",
      "Current iteration=3100, loss=0.25032297008366505\n",
      "Current iteration=3200, loss=0.2503207555149848\n",
      "Current iteration=3300, loss=0.25031865038423684\n",
      "Current iteration=3400, loss=0.25031664578293883\n",
      "Current iteration=3500, loss=0.2503147337721345\n",
      "Current iteration=3600, loss=0.250312907263529\n",
      "Current iteration=3700, loss=0.25031115991618114\n",
      "Current iteration=3800, loss=0.25030948604658415\n",
      "Current iteration=3900, loss=0.25030788055029496\n",
      "Current iteration=4000, loss=0.25030633883354236\n",
      "Current iteration=4100, loss=0.25030485675347447\n",
      "Current iteration=4200, loss=0.2503034305658938\n",
      "Current iteration=4300, loss=0.2503020568794965\n",
      "Current iteration=4400, loss=0.25030073261576385\n",
      "Current iteration=4500, loss=0.25029945497377437\n",
      "Current iteration=4600, loss=0.25029822139930374\n",
      "Current iteration=4700, loss=0.2502970295576658\n",
      "Current iteration=4800, loss=0.250295877309818\n",
      "Current iteration=4900, loss=0.25029476269132284\n",
      "loss=0.25029369451005384\n"
     ]
    }
   ],
   "source": [
    "max_iters = 5000\n",
    "gamma = 0.5\n",
    "\n",
    "# Build tx\n",
    "tx_tr = np.c_[np.ones((y_tr.shape[0], 1)), x_tr]\n",
    "initial_w = np.zeros((tx_tr.shape[1], 1))\n",
    "\n",
    "# Binary classification using logistic regression\n",
    "w, loss = logistic_regression(y_tr, tx_tr, initial_w, max_iters, gamma = 0.5)\n",
    "\n",
    "# Regularized binary classification using logistic regression\n",
    "# lambda_ = 10e-4\n",
    "# w_reg,loss_reg = reg_logistic_regression(y_tr, tx_tr, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute accuracy and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_te = np.c_[np.ones((x_te.shape[0], 1)), x_te]\n",
    "y_te = np.expand_dims(y_te, 1)\n",
    "y_pred = tx_te.dot(w)\n",
    "\n",
    "# Compute accuracy of training\n",
    "compute_accuracy(y_te,y_pred)\n",
    "\n",
    "# Compute F1-score of training\n",
    "compute_f1_score(y_te, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "\n",
    "xt = replace_nan_by_mean(x_test)\n",
    "xt_filtered = filtering(xt, test_data_path)\n",
    "xt_standardized = standardize(xt_filtered)\n",
    "xtest = np.c_[np.ones((xt.shape[0], 1)), xt_standardized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = apply_model(xtest, w)\n",
    "predictions = np.where(predictions==0,-1, predictions)\n",
    "\n",
    "create_csv_submission(test_ids, predictions, 'predictions_name.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
